@misc{shao_nonlinear_2023,
	title = {Nonlinear {Correct} and {Smooth} for {Semi}-{Supervised} {Learning}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/2310.05757},
	abstract = {Graph-based semi-supervised learning (GSSL) has been used successfully in various applications. Existing methods leverage the graph structure and labeled samples for classification. Label Propagation (LP) and Graph Neural Networks (GNNs) both iteratively pass messages on graphs, where LP propagates node labels through edges and GNN aggregates node features from the neighborhood. Recently, combining LP and GNN has led to improved performance. However, utilizing labels and features jointly in higher-order graphs has not been explored. Therefore, we propose Nonlinear Correct and Smooth (NLCS), which improves the existing post-processing approach by incorporating non-linearity and higher-order representation into the residual propagation to handle intricate node relationships effectively. Systematic evaluations show that our method achieves remarkable average improvements of 13.71\% over base prediction and 2.16\% over the state-of-the-art post-processing method on six commonly used datasets. Comparisons and analyses show our method effectively utilizes labels and features jointly in higher-order graphs to resolve challenging graph relationships.},
	urldate = {2024-04-15},
	publisher = {arXiv},
	author = {Shao, Yuanhang and Liu, Xiuwen},
	month = oct,
	year = {2023},
	note = {arXiv:2310.05757 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\yuanhang shao\\Zotero\\storage\\EVYCIFH4\\Shao and Liu - 2023 - Nonlinear Correct and Smooth for Semi-Supervised L.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\yuanhang shao\\Zotero\\storage\\IU89XPKU\\2310.html:text/html},
}

@misc{mandal_automatic_2023,
	title = {Automatic {Historical} {Stock} {Price} {Dataset} {Generation} {Using} {Python}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/2308.13414},
	abstract = {With the dynamic political and economic environments, the ever-changing stock markets generate large amounts of data daily. Acquiring up-to-date data is crucial to enhancing predictive precision in stock price behavior studies. However, preparing the dataset manually can be challenging and time-demanding. The stock market analysis usually revolves around specific indices such as S\&P500, Nasdaq, Dow Jones, the New York Stock Exchange (NYSE), etc. It is necessary to analyze all the companies of any particular index. While raw data are accessible from diverse financial websites, these resources are tailored for individual company data retrieval and there is a big gap between what is available and what is needed to generate large datasets. Python emerges as a valuable tool for comprehensively collecting all constituent stocks within a given index. While certain online sources offer code snippets for limited dataset generation, a comprehensive and unified script is yet to be developed and publicly available. Therefore, we present a comprehensive and consolidated code resource that facilitates the extraction of updated datasets for any particular time period and for any specific stock market index and closes the gap. The code is available at https://github.com/amp1590/automatic\_stock\_data\_collection.},
	urldate = {2024-04-15},
	publisher = {arXiv},
	author = {Mandal, Arunima and Shao, Yuanhang and Liu, Xiuwen},
	month = aug,
	year = {2023},
	note = {arXiv:2308.13414 [cs]},
	keywords = {Computer Science - Computational Engineering, Finance, and Science},
	file = {arXiv Fulltext PDF:C\:\\Users\\yuanhang shao\\Zotero\\storage\\355226DX\\Mandal et al. - 2023 - Automatic Historical Stock Price Dataset Generatio.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\yuanhang shao\\Zotero\\storage\\FS76SFDK\\2308.html:text/html},
}

@misc{shao_rels-dqn_2023,
	title = {{RELS}-{DQN}: {A} {Robust} and {Efficient} {Local} {Search} {Framework} for {Combinatorial} {Optimization}},
	copyright = {All rights reserved},
	shorttitle = {{RELS}-{DQN}},
	url = {http://arxiv.org/abs/2304.06048},
	abstract = {Combinatorial optimization (CO) aims to efficiently find the best solution to NP-hard problems ranging from statistical physics to social media marketing. A wide range of CO applications can benefit from local search methods because they allow reversible action over greedy policies. Deep Q-learning (DQN) using message-passing neural networks (MPNN) has shown promise in replicating the local search behavior and obtaining comparable results to the local search algorithms. However, the over-smoothing and the information loss during the iterations of message passing limit its robustness across applications, and the large message vectors result in memory inefficiency. Our paper introduces RELS-DQN, a lightweight DQN framework that exhibits the local search behavior while providing practical scalability. Using the RELS-DQN model trained on one application, it can generalize to various applications by providing solution values higher than or equal to both the local search algorithms and the existing DQN models while remaining efficient in runtime and memory.},
	urldate = {2024-04-15},
	publisher = {arXiv},
	author = {Shao, Yuanhang and Dey, Tonmoy and Vuckovic, Nikola and Van Popering, Luke and Kuhnle, Alan},
	month = apr,
	year = {2023},
	note = {arXiv:2304.06048 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\yuanhang shao\\Zotero\\storage\\XPPAEET7\\Shao et al. - 2023 - RELS-DQN A Robust and Efficient Local Search Fram.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\yuanhang shao\\Zotero\\storage\\2LZUFZ47\\2304.html:text/html},
}

@inproceedings{shao_segmentation_2019,
	title = {Segmentation and {Abstraction} of an {IoT} {Enabled} {Distributed} {Sensor} {Network}},
	copyright = {All rights reserved},
	url = {https://ieeexplore.ieee.org/abstract/document/8651869/},
	urldate = {2024-04-15},
	booktitle = {2019 16th {IEEE} {Annual} {Consumer} {Communications} \& {Networking} {Conference} ({CCNC})},
	publisher = {IEEE},
	author = {Shao, Yuanhang and Kumar, Suman and Kawakami, Takahiro},
	year = {2019},
	pages = {1--7},
}
